# # torch #==2.6.0
wheel
packaging
# flash-attn sometimes needs to be installed with pip build isolation disabled
# so the build can import your existing torch. If install fails, use:
#   pip install -r requirements.txt --no-build-isolation
flash-attn
vllm #==0.8.5
transformers #==4.52.3
accelerate #==1.7.0
datasets #==3.6.0
deepspeed #==0.16.4
wandb #==0.19.11
ipykernel #==6.29.5
ipywidgets #==8.1.7
jupyter #==1.1.1

# ???
# torch==2.6.0
# wheel
# packaging
# # ???
# flash-attn==2.7.4.post1
# vllm==0.8.5
# transformers==4.52.3
# accelerate==1.7.0
# datasets==3.6.0
# deepspeed==0.16.4
# wandb==0.19.11
# ipykernel==6.29.5
# ipywidgets==8.1.7
# jupyter==1.1.1
# # fsspec==2025.12.0
